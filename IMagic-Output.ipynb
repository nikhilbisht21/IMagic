{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMagic\n",
    "\n",
    "<p>\n",
    "IMagic is a Deep Learning based model to color a black and white image i.e. by converting a Grayscale image into RGB image\n",
    "</p>\n",
    "### Python libraries used\n",
    "<br>\n",
    "<dl>\n",
    "    <dt>Tensorflow</dt>\n",
    "    <dd><div style=\"margin-left:20px;\">Tensorflow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as Neural Networks.</div></dd>\n",
    "    <dt>Numpy</dt>\n",
    "    <dd><div style=\"margin-left:20px;\">NumPy is the fundamental package for scientific computing with Python. NumPy can also be used as an efficient multi-dimensional container of generic data. </div></dd>\n",
    "    <dt>Open-cv</dt>\n",
    "    <dd><div style=\"margin-left:20px;\">OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at real-time computer vision. Used as an image processing library in this program.</div></dd>\n",
    "    <dt>Matplotlib</dt>\n",
    "    <dd><div style=\"margin-left:20px;\">Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. The matplotlib.pyplot module contains functions that allow you to generate many kinds of plots quickly.</div></dd>\n",
    "    <dt>h5py</dt>\n",
    "    <dd><div style=\"margin-left:20px;\">The h5py package is a Pythonic interface to the HDF5 binary data format.\n",
    "\n",
    "It lets you store huge amounts of numerical data, and easily manipulate that data from NumPy. For example, you can slice into multi-terabyte datasets stored on disk, as if they were real NumPy arrays. Thousands of datasets can be stored in a single file, categorized and tagged however you want.</div></dd>\n",
    "</dl>\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The datasets are image files that can be read using the open-cv python library. Each sample image used was originally 256x256 pixels and consists of 3 bands - red, green, and blue. These images are resized into 64x64 pixels images and converted seperately into Grayscale image which could be used as input and orginal 3 bands image as an output to neural network. \n",
    "\n",
    "\n",
    "\n",
    "### Methodology\n",
    "\n",
    "This model follows the methodology of Supervised Learning using Regression where we have to predict a particular value for input features.</b><br>\n",
    "Neural network used for converting grayscale image to RGB image takes a 64X64 pixels grayscale image as input and 64x64x3 pixels RGB image as an output. These images are converted into single vertor before feeding into the neural network. Then model is trained over these images repeatedly to achieve the minimal loss and highest accuracy.\n",
    "\n",
    "<div style=\"height:100px;width:200px;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Tensorflow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LICHfvrNy5Bi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading,Resizing  and flattening the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 231,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1386,
     "status": "error",
     "timestamp": 1522093514927,
     "user": {
      "displayName": "Rude Boy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117813436979060160370"
     },
     "user_tz": -330
    },
    "id": "54-NO4ZRy4-S",
    "outputId": "a7b9c30e-6833-46fd-9ef7-a72e04b29fb9"
   },
   "outputs": [],
   "source": [
    "mn=721\n",
    "Y=np.zeros((36237,12288))\n",
    "X=np.zeros((36237,4096))\n",
    "k=0\n",
    "\n",
    "dir='datac/dataa/train/'\n",
    "for filename in os.listdir(dir):\n",
    "    #file='res/image1/image{0}.jpg'.format(var+1)\n",
    "    img=cv2.imread(dir+filename)\n",
    "    img=cv2.resize(img,(64,64))\n",
    "    im=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY).reshape(1,-1)\n",
    "    X[k]=im.reshape(1,-1)\n",
    "    Y[k]=img.reshape(1,64*64*3)\n",
    "    k=k+1\n",
    "    if k==721:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing the dataset into Training and Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1284,
     "status": "ok",
     "timestamp": 1522093365064,
     "user": {
      "displayName": "Rude Boy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117813436979060160370"
     },
     "user_tz": -330
    },
    "id": "8QJf2z7IzGe3",
    "outputId": "9c496cce-7665-458d-969a-2fa893316683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train if of size (4096, 648)\n",
      "Y_train if of size (12288, 648)\n",
      "X_test if of size (4096, 73)\n",
      "Y_test if of size (12288, 73)\n",
      "Total examples : 721\n"
     ]
    }
   ],
   "source": [
    "nm=721\n",
    "m=int(np.floor(90*nm/100))\n",
    "\n",
    "X_train=X[0:m,:].T\n",
    "Y_train=Y[0:m,:].T\n",
    "X_test=X[m:nm,:].T\n",
    "Y_test=Y[m:nm,:].T\n",
    "print('X_train if of size {}'.format(X_train.shape))\n",
    "print('Y_train if of size {}'.format(Y_train.shape))\n",
    "print('X_test if of size {}'.format(X_test.shape))\n",
    "print('Y_test if of size {}'.format(Y_test.shape))\n",
    "print('Total examples : {}'.format(Y_test.shape[1]+Y_train.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating placeholder to pass training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cykJ5AHTzJ5_"
   },
   "outputs": [],
   "source": [
    "def create_placeholder(n_x,n_y):\n",
    "    X=tf.placeholder(dtype=tf.float32,shape=[n_x,None],name='X')\n",
    "    Y=tf.placeholder(dtype=tf.float32,shape=[n_y,None],name='Y')\n",
    "    \n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_parameters(param):\n",
    "    \n",
    "    W1=tf.Variable(param['W1'],[12288,4096],dtype=tf.float32)\n",
    "    b1=tf.Variable(param['b1'],[12288,1],dtype=tf.float32)\n",
    "    W2=tf.Variable(param['W2'],[12288,12288],dtype=tf.float32)\n",
    "    b2=tf.Variable(param['b2'],[12288,1],dtype=tf.float32)\n",
    "    \n",
    "    #print(type(param['W1']))\n",
    "    #W1=param['W1'],\n",
    "    #b1=param['b1'],\n",
    "    #W2=param['W2'],\n",
    "    #b2=param['b2']\n",
    "    \n",
    "    parameters={\n",
    "        \"W1\":W1,\n",
    "        \"b1\":b1,\n",
    "        \"W2\":W2,\n",
    "        \"b2\":b2\n",
    "    }\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intializing the parameters i.e. defining the weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "V1yhjSuIzLyw"
   },
   "outputs": [],
   "source": [
    "def parameters():\n",
    "    \n",
    "    #Total no. of layers in network : 3\n",
    "    #1 input layer, 1 hidden layer, 1 output layer\n",
    "    \n",
    "    #Size of layer 1 : 4096\n",
    "    #Size of layer 2: 12288\n",
    "    #Size of layer 3: 12288\n",
    "    \n",
    "    #W1(Weights of input layer) : [12288,4096]\n",
    "    #b1(bias of input layer) : [12288,1]\n",
    "    #W2(Weights of hidden layer) : [12288,12288]\n",
    "    #b2(bias of hidden layer) : [12288,1]\n",
    "    \n",
    "    \n",
    "    W1=tf.get_variable(\"W1\",[12288,4096],initializer=tf.contrib.layers.xavier_initializer(seed=1),dtype=tf.float32)\n",
    "    b1=tf.get_variable(\"b1\",[12288,1],initializer=tf.contrib.layers.xavier_initializer(seed=1),dtype=tf.float32)\n",
    "    W2=tf.get_variable(\"W2\",[12288,12288],initializer=tf.contrib.layers.xavier_initializer(seed=1),dtype=tf.float32)\n",
    "    b2=tf.get_variable(\"b2\",[12288,1],initializer=tf.contrib.layers.xavier_initializer(seed=1),dtype=tf.float32)\n",
    "    \n",
    "    parameters={\n",
    "        \"W1\":W1,\n",
    "        \"b1\":b1,\n",
    "        \"W2\":W2,\n",
    "        \"b2\":b2\n",
    "    }\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "28FiTTgVzOC-"
   },
   "outputs": [],
   "source": [
    "def forward(X,parameters):\n",
    "    \n",
    "    W1=parameters['W1']\n",
    "    b1=parameters['b1']\n",
    "    W2=parameters['W2']\n",
    "    b2=parameters['b2']\n",
    "   \n",
    "    A1=tf.add(tf.matmul(W1,X),b1)\n",
    "    A2=tf.add(tf.matmul(W2,A1),b2)\n",
    "    \n",
    "    return A2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g_HSlHCgzP3q"
   },
   "outputs": [],
   "source": [
    "def get_cost(A2,Y):\n",
    "    x=tf.transpose(A2)\n",
    "    y=tf.transpose(Y)\n",
    "    \n",
    "    cost=tf.reduce_mean(tf.square(x-y))\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating batch for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qzl6hslHy45y"
   },
   "outputs": [],
   "source": [
    "def batching(X,Y,batch_size=64,seed=0):\n",
    "    \n",
    "    m=X.shape[1]\n",
    "    mini_batches=[]\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #Permutate the training data\n",
    "    per=np.random.permutation(m)\n",
    "    x_shuffled=X[:,per]\n",
    "    y_shuffled=Y[:,per]\n",
    "    \n",
    "    #partitioning\n",
    "    batches=int(np.floor(m/batch_size))\n",
    "    \n",
    "    for var in range(batches):\n",
    "        batch_x=x_shuffled[:,var*batch_size:var*batch_size+batch_size]\n",
    "        batch_y=y_shuffled[:,var*batch_size:var*batch_size+batch_size]\n",
    "        mini_batches.append((batch_x,batch_y))\n",
    "        \n",
    "    if m%batch_size!=0:\n",
    "        batch_x=x_shuffled[:,batches*batch_size:m]\n",
    "        batch_y=y_shuffled[:,batches*batch_size:m]\n",
    "        mini_batches.append((batch_x,batch_y))\n",
    "        #print(len(mini_batches))\n",
    "    \n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e6c-m-p2y4t5"
   },
   "outputs": [],
   "source": [
    "def model(X_train,Y_train,X_test,Y_test,learning_rate=0.000001,epochs=50,mini_batch_size=32,param=None):\n",
    "    \n",
    "    jl=0\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    seed=3\n",
    "    (n_x,m)=X_train.shape\n",
    "    (n_y,m)=Y_train.shape\n",
    "    \n",
    "    #create palceholders \n",
    "    X,Y=create_placeholder(n_x,n_y)\n",
    "    costs=[]\n",
    "\n",
    "    #para=parameters()\n",
    "    para=fetch_parameters(param)\n",
    "    \n",
    "    A2=forward(X,para)\n",
    "    \n",
    "    cost=get_cost(A2,Y)\n",
    "    \n",
    "    #Backpropagation\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    #Initialize all variables\n",
    "    init=tf.global_variables_initializer()\n",
    "    \n",
    "    #Start session\n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        #Run session\n",
    "        session.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            var=epoch\n",
    "            \n",
    "            if var!=0:\n",
    "                os.remove('track_{}.txt'.format(str(var)))\n",
    "           \n",
    "            fp=open('track_{}.txt'.format(str(var+1)),'w')\n",
    "            fp.write('Iteration no. {}\\r\\n'.format(var+1))\n",
    "            fp.close()\n",
    "            \n",
    "            num_minibatches = int(m / mini_batch_size)\n",
    "            epoch_cost=0\n",
    "            seed=seed+1\n",
    "            minibatches=batching(X_train,Y_train,mini_batch_size,seed)\n",
    "            \n",
    "            kl=1\n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                #Select a batch\n",
    "                (mini_X,mini_y)=minibatch\n",
    "                \n",
    "                #Main Line to run the code\n",
    "                _, mini_cost=session.run([optimizer, cost], feed_dict={X:mini_X,Y:mini_y})\n",
    "                \n",
    "                epoch_cost+=mini_cost/num_minibatches\n",
    "                \n",
    "                if jl!=0:\n",
    "                    os.remove('track_b{}.txt'.format(str(jl)))\n",
    "           \n",
    "                fp=open('track_b{}.txt'.format(str(kl)),'w')\n",
    "                fp.write('Batch no. {0}\\r\\nEpoch cost {1}'.format(kl,epoch_cost))\n",
    "                fp.close()\n",
    "                jl=kl\n",
    "                #kl=kl+1\n",
    "                \n",
    "            if epoch % 1 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                univCost.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        # lets save the parameters in a variable\n",
    "        parameter = session.run(para)\n",
    "        print (\"Parameters have been trained!\")\n",
    "                \n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(A2), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameter,costs        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the pre-trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={}\n",
    "filename = 'params1.hdf5'\n",
    "f = h5py.File(filename, 'r')\n",
    "param['W1']=f['w1'][()]\n",
    "param['b1']=f['b1'][()]\n",
    "param['W2']=f['w2'][()]\n",
    "param['b2']=f['b2'][()]\n",
    "univCost=list(f['cost'][()])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1153,
     "status": "error",
     "timestamp": 1522093502516,
     "user": {
      "displayName": "Rude Boy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117813436979060160370"
     },
     "user_tz": -330
    },
    "id": "SlClt8ZJzYXn",
    "outputId": "c7c27cf2-5a3b-4bd5-ba6b-bbc4063ee3f0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 10.427301\n",
      "Cost after epoch 1: 9.253550\n",
      "Cost after epoch 2: 8.915696\n",
      "Cost after epoch 3: 9.778020\n",
      "Cost after epoch 4: 9.174371\n",
      "Cost after epoch 5: 9.297854\n",
      "Cost after epoch 6: 9.224076\n",
      "Cost after epoch 7: 9.256226\n",
      "Cost after epoch 8: 8.966103\n",
      "Cost after epoch 9: 11.712112\n",
      "Cost after epoch 10: 9.298902\n",
      "Cost after epoch 11: 9.022528\n",
      "Cost after epoch 12: 8.968265\n",
      "Cost after epoch 13: 8.881223\n",
      "Cost after epoch 14: 8.931230\n",
      "Cost after epoch 15: 8.773939\n",
      "Cost after epoch 16: 8.996440\n",
      "Cost after epoch 17: 9.053765\n",
      "Cost after epoch 18: 8.912380\n",
      "Cost after epoch 19: 8.914247\n",
      "Cost after epoch 20: 9.179793\n",
      "Cost after epoch 21: 9.049880\n",
      "Cost after epoch 22: 9.416660\n",
      "Cost after epoch 23: 9.338563\n",
      "Cost after epoch 24: 9.307692\n",
      "Cost after epoch 25: 9.845260\n",
      "Cost after epoch 26: 9.026983\n",
      "Cost after epoch 27: 8.720493\n",
      "Cost after epoch 28: 8.801162\n",
      "Cost after epoch 29: 9.086967\n"
     ]
    }
   ],
   "source": [
    "param,costs = model(X_train, Y_train, X_train, Y_train,param=param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 231,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2016,
     "status": "error",
     "timestamp": 1522092900127,
     "user": {
      "displayName": "Rude Boy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117813436979060160370"
     },
     "user_tz": -330
    },
    "id": "ah1BBwc7zbKM",
    "outputId": "5061ba1a-6364-4452-89f6-1f3ae015b908"
   },
   "outputs": [],
   "source": [
    "univCost.append(404)\n",
    "f=h5py.File('params1.hdf5','w')\n",
    "f.create_dataset('w1',data=np.array(param['W1']))\n",
    "f.create_dataset('w2',data=np.array(param['W2']))\n",
    "f.create_dataset('b1',data=np.array(param['b1']))\n",
    "f.create_dataset('b2',data=np.array(param['b2']))\n",
    "f.create_dataset('cost',data=np.array(univCost))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "test.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
